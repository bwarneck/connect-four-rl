{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Four DQN Training Visualization\n",
    "\n",
    "This notebook trains a Deep Q-Network agent to play Connect Four and visualizes the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from game import ConnectFour, RandomAgent\n",
    "from agent import DQNAgent\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the Agent\n",
    "\n",
    "Create a DQN agent with the specified hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(\n",
    "    learning_rate=0.001,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.9998,\n",
    "    buffer_size=100000,\n",
    "    batch_size=64,\n",
    "    target_update_freq=1000\n",
    ")\n",
    "\n",
    "print(f\"Agent initialized\")\n",
    "print(f\"Device: {agent.device}\")\n",
    "print(f\"Initial epsilon: {agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Loop with Live Visualization\n",
    "\n",
    "Train the agent through self-play and track statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_stats(agent, num_episodes=10000, print_every=500):\n",
    "    \"\"\"Train agent and collect statistics.\"\"\"\n",
    "    stats = {\n",
    "        'episode': [],\n",
    "        'p1_win_rate': [],\n",
    "        'p2_win_rate': [],\n",
    "        'draw_rate': [],\n",
    "        'epsilon': [],\n",
    "        'avg_game_length': [],\n",
    "        'avg_loss': []\n",
    "    }\n",
    "    \n",
    "    # Rolling counters\n",
    "    p1_wins = p2_wins = draws = total_moves = 0\n",
    "    losses = []\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        game = ConnectFour()\n",
    "        episode_transitions = []\n",
    "        \n",
    "        while not game.done:\n",
    "            state = game.get_state()\n",
    "            legal_actions = game.get_legal_actions()\n",
    "            current_player = game.current_player\n",
    "            \n",
    "            action = agent.choose_action(state, legal_actions, training=True)\n",
    "            next_state, reward, done = game.make_move(action)\n",
    "            \n",
    "            episode_transitions.append({\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'player': current_player,\n",
    "                'next_state': next_state,\n",
    "                'done': done\n",
    "            })\n",
    "        \n",
    "        total_moves += game.move_count\n",
    "        \n",
    "        # Assign rewards and train\n",
    "        for i, t in enumerate(episode_transitions):\n",
    "            if game.winner == t['player']:\n",
    "                reward = 1.0\n",
    "                if t['player'] == ConnectFour.PLAYER_1:\n",
    "                    p1_wins += 1\n",
    "                else:\n",
    "                    p2_wins += 1\n",
    "            elif game.winner is None:\n",
    "                reward = 0.0\n",
    "                if i == len(episode_transitions) - 1:\n",
    "                    draws += 1\n",
    "            else:\n",
    "                reward = -1.0\n",
    "            \n",
    "            agent.store_transition(t['state'], t['action'], reward, t['next_state'], t['done'])\n",
    "            loss = agent.train_step()\n",
    "            if loss is not None:\n",
    "                losses.append(loss)\n",
    "        \n",
    "        agent.episodes_trained += 1\n",
    "        \n",
    "        # Record stats\n",
    "        if episode % print_every == 0:\n",
    "            total_games = p1_wins + p2_wins + draws\n",
    "            if total_games > 0:\n",
    "                stats['episode'].append(episode)\n",
    "                stats['p1_win_rate'].append(p1_wins / total_games * 100)\n",
    "                stats['p2_win_rate'].append(p2_wins / total_games * 100)\n",
    "                stats['draw_rate'].append(draws / total_games * 100)\n",
    "                stats['epsilon'].append(agent.epsilon)\n",
    "                stats['avg_game_length'].append(total_moves / total_games)\n",
    "                stats['avg_loss'].append(np.mean(losses) if losses else 0)\n",
    "                \n",
    "                print(f\"Episode {episode:6d} | \"\n",
    "                      f\"P1: {stats['p1_win_rate'][-1]:5.1f}% | \"\n",
    "                      f\"P2: {stats['p2_win_rate'][-1]:5.1f}% | \"\n",
    "                      f\"Draw: {stats['draw_rate'][-1]:5.1f}% | \"\n",
    "                      f\"Eps: {agent.epsilon:.3f} | \"\n",
    "                      f\"Loss: {stats['avg_loss'][-1]:.4f}\")\n",
    "            \n",
    "            # Reset counters\n",
    "            p1_wins = p2_wins = draws = total_moves = 0\n",
    "            losses = []\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "NUM_EPISODES = 20000\n",
    "PRINT_EVERY = 500\n",
    "\n",
    "print(f\"Training for {NUM_EPISODES} episodes...\\n\")\n",
    "stats = train_with_stats(agent, num_episodes=NUM_EPISODES, print_every=PRINT_EVERY)\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Progress Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Win/Loss/Draw rates\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(stats['episode'], stats['p1_win_rate'], label='Player 1 (Red)', color='red', alpha=0.8)\n",
    "ax1.plot(stats['episode'], stats['p2_win_rate'], label='Player 2 (Yellow)', color='gold', alpha=0.8)\n",
    "ax1.plot(stats['episode'], stats['draw_rate'], label='Draws', color='gray', alpha=0.8)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Rate (%)')\n",
    "ax1.set_title('Self-Play Outcomes Over Training')\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "# Epsilon decay\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(stats['episode'], stats['epsilon'], color='blue', linewidth=2)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Epsilon')\n",
    "ax2.set_title('Exploration Rate (Epsilon) Decay')\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Average game length\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(stats['episode'], stats['avg_game_length'], color='green', linewidth=2)\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Moves')\n",
    "ax3.set_title('Average Game Length')\n",
    "ax3.axhline(y=21, color='red', linestyle='--', alpha=0.5, label='Min possible (21 moves)')\n",
    "ax3.legend()\n",
    "\n",
    "# Training loss\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(stats['episode'], stats['avg_loss'], color='purple', linewidth=2)\n",
    "ax4.set_xlabel('Episode')\n",
    "ax4.set_ylabel('Loss')\n",
    "ax4.set_title('Average Training Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../training_progress.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Against Random Opponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vs_random(agent, num_games=1000):\n",
    "    \"\"\"Evaluate agent against random opponent.\"\"\"\n",
    "    random_agent = RandomAgent()\n",
    "    results = {'as_p1': {'wins': 0, 'losses': 0, 'draws': 0},\n",
    "               'as_p2': {'wins': 0, 'losses': 0, 'draws': 0}}\n",
    "    \n",
    "    for game_num in range(num_games):\n",
    "        game = ConnectFour()\n",
    "        agent_is_p1 = (game_num % 2 == 0)\n",
    "        key = 'as_p1' if agent_is_p1 else 'as_p2'\n",
    "        \n",
    "        while not game.done:\n",
    "            state = game.get_state()\n",
    "            legal = game.get_legal_actions()\n",
    "            \n",
    "            if (game.current_player == ConnectFour.PLAYER_1) == agent_is_p1:\n",
    "                action = agent.choose_action(state, legal, training=False)\n",
    "            else:\n",
    "                action = random_agent.choose_action(state, legal)\n",
    "            \n",
    "            game.make_move(action)\n",
    "        \n",
    "        if game.winner is None:\n",
    "            results[key]['draws'] += 1\n",
    "        elif (game.winner == ConnectFour.PLAYER_1) == agent_is_p1:\n",
    "            results[key]['wins'] += 1\n",
    "        else:\n",
    "            results[key]['losses'] += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluating against random opponent (1000 games)...\")\n",
    "eval_results = evaluate_vs_random(agent, 1000)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for role in ['as_p1', 'as_p2']:\n",
    "    r = eval_results[role]\n",
    "    total = r['wins'] + r['losses'] + r['draws']\n",
    "    role_name = \"As Player 1 (Red)\" if role == 'as_p1' else \"As Player 2 (Yellow)\"\n",
    "    print(f\"\\n{role_name}:\")\n",
    "    print(f\"  Wins:   {r['wins']/total*100:5.1f}% ({r['wins']}/{total})\")\n",
    "    print(f\"  Losses: {r['losses']/total*100:5.1f}% ({r['losses']}/{total})\")\n",
    "    print(f\"  Draws:  {r['draws']/total*100:5.1f}% ({r['draws']}/{total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Agent's Decision Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_values(agent, game):\n",
    "    \"\"\"Visualize Q-values for current board state.\"\"\"\n",
    "    state = game.get_state()\n",
    "    legal_actions = game.get_legal_actions()\n",
    "    \n",
    "    # Get Q-values from network\n",
    "    with torch.no_grad():\n",
    "        state_tensor = agent._preprocess_state(state, game.current_player)\n",
    "        q_values = agent.policy_net(state_tensor).cpu().numpy()[0]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Board visualization\n",
    "    board_display = np.zeros((6, 7, 3))\n",
    "    for r in range(6):\n",
    "        for c in range(7):\n",
    "            if state[r, c] == 1:\n",
    "                board_display[r, c] = [1, 0, 0]  # Red\n",
    "            elif state[r, c] == 2:\n",
    "                board_display[r, c] = [1, 1, 0]  # Yellow\n",
    "            else:\n",
    "                board_display[r, c] = [0.9, 0.9, 0.9]  # Empty\n",
    "    \n",
    "    ax1.imshow(board_display)\n",
    "    ax1.set_xticks(range(7))\n",
    "    ax1.set_yticks(range(6))\n",
    "    ax1.set_title(f'Current Board (Player {game.current_player} to move)')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Q-values bar chart\n",
    "    colors = ['green' if i in legal_actions else 'red' for i in range(7)]\n",
    "    ax2.bar(range(7), q_values, color=colors, alpha=0.7)\n",
    "    ax2.set_xlabel('Column')\n",
    "    ax2.set_ylabel('Q-Value')\n",
    "    ax2.set_title('Q-Values by Column (green=legal, red=illegal)')\n",
    "    ax2.set_xticks(range(7))\n",
    "    \n",
    "    # Highlight best legal action\n",
    "    best_action = max(legal_actions, key=lambda a: q_values[a])\n",
    "    ax2.axvline(x=best_action, color='blue', linestyle='--', linewidth=2, label=f'Best: col {best_action}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Need torch import for visualization\n",
    "import torch\n",
    "\n",
    "# Create a sample game state\n",
    "game = ConnectFour()\n",
    "# Make some moves to create an interesting position\n",
    "for col in [3, 3, 4, 4, 2]:\n",
    "    if col in game.get_legal_actions():\n",
    "        game.make_move(col)\n",
    "\n",
    "print(\"Current board:\")\n",
    "print(game)\n",
    "print()\n",
    "visualize_q_values(agent, game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained agent\n",
    "agent.save('../trained_agent.pth')\n",
    "print(\"Agent saved to ../trained_agent.pth\")\n",
    "\n",
    "# Print final stats\n",
    "final_stats = agent.get_stats()\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"  Episodes trained: {final_stats['episodes']}\")\n",
    "print(f\"  Training steps: {final_stats['training_steps']}\")\n",
    "print(f\"  Final epsilon: {final_stats['epsilon']:.4f}\")\n",
    "print(f\"  Replay buffer size: {final_stats['buffer_size']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
